{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[leiden algo documentation](https://leidenalg.readthedocs.io/en/latest/install.html)\n",
    "\n",
    "# General Approach\n",
    "\n",
    "In this analysis, we explore the clustering of scientific publications using the Leiden algorithm, with a focus on adjusting the algorithm's parameters to improve the coherence and relevance of the clusters formed. The Leiden algorithm, which improves upon the limitations of modularity optimization seen in previous methods, is employed for its effectiveness in identifying communities within networks. We particularly experiment with various resolution parameters, iterating through a range of values to tune the granularity of the clustering results. The objective is to find an optimal balance where clusters are neither too broad, encompassing multiple unrelated topics, nor too narrow, breaking down coherent subjects into excessive subgroups.\n",
    "\n",
    "The analysis starts with the application of the Leiden algorithm using default settings and then progresses to more sophisticated implementations, including adjusting the resolution parameter to influence the size and coherence of clusters. This parameter tuning is crucial for addressing the resolution limit problem inherent in modularity-based community detection methods. By carefully selecting resolution parameters, we aim to achieve a set of clusters that are meaningful and manageable for qualitative analysis.\n",
    "\n",
    "This process is supported by the creation of summary sheets and detailed exploratory sheets for each parameter set, facilitating a thorough review of the clustering outcomes. Parameters leading to the most coherent and distinct clusters, based on predefined decision criteria, are identified and selected for further analysis. The final selection of parameters is based on qualitative assessments of cluster coherence, ensuring that the clusters formed are both scientifically relevant and insightful for further research explorations.\n",
    "\n",
    "Notes about the leiden algorithm:\n",
    "\n",
    "### The most simpe implementation is this:\n",
    "\n",
    "Here we would just use all default parameters.\n",
    "The metrices are good, the qualitative assessment poor.\n",
    "The `ModularityVertexPartition` is the default partitioning algorithm\n",
    "\n",
    "- it is based on the modularity measure (link density within communities vs. link density between communities)\n",
    "- Higher scires = stronger division into communities\n",
    "- It tries to maximize the modularity score\n",
    "\n",
    "The problem here is that the partitions are really big (if run without max_comm_size, around 5,000 papers in the largest communit)\n",
    "Examining the most central articles (based on the eigenvector similarity), they do not seem coherent.\n",
    "\n",
    "**PROBLEM:** Modularity, though\n",
    "robust for many practical applications, suffers from the resolution limit problem,\n",
    "in which optimization may fail to identify clusters smaller than a certain scale\n",
    "that is dependent on properties of the network. (https://arxiv.org/pdf/2308.09644.pdf)\n",
    "\n",
    "### The second implementation is this:\n",
    "\n",
    "We are trying to tune the resolution parameter. To access it, we use the CPMVertexPartition algorithm. (CPM = Constant Potts Model)\n",
    "This means that modularity is not an important parameter anymore.\n",
    "\n",
    "- The resolution parameter is the only parameter of this algorithm\n",
    "- It compares the actual link density to the resolution parameter\n",
    "- If the link density is higher than the resolution parameter, the nodes are put into the same community\n",
    "- It has a quality attribute, which is similar to the modularity score\n",
    "- We will not rely on this but use a qualitative analysis to assess the results\n",
    "\n",
    "In previous papers (Waltman 2020a, Ahlgren 2020), the authors used different levels of granularity. They used 10 different values of the resolution parameter γ: 0.000001, 0.000002, 0.000005, 0.00001, 0.00002, 0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01\n",
    "\n",
    "We decide to use the following resolution parameter to narrow down our search.\n",
    "0.00001,0.00002,0.0001,0.0002,0.001,0.002,0.01,0.02,\n",
    "\n",
    "We then evaluated the solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import leidenalg as la\n",
    "import igraph as ig\n",
    "import sys\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"/Users/jlq293/Projects/Study-1-Bibliometrics/src/network/\")\n",
    "from PartitionCreator import PartitionCreator\n",
    "from NetworkAnalyzer import CommunityExplorer, FullExplorer\n",
    "from NetworkAnalyzerUtils import NetworkAnalyzerUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load df and graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alpha0.3_k10', 'alpha0.3_k15', 'alpha0.3_k20', 'alpha0.3_k5', 'alpha0.5_k10', 'alpha0.5_k15', 'alpha0.5_k20', 'alpha0.5_k5'])\n"
     ]
    }
   ],
   "source": [
    "# load all files in a dictionary with params as keys and graph as values\n",
    "\n",
    "params_graph_dict, df = NetworkAnalyzerUtils().load_graph_files()\n",
    "\n",
    "print(params_graph_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPM RESOLUTION DETERMINATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# waltman 2020a:\n",
    "\n",
    "Different levels of granularity were considered. For each relatedness measure, we obtained 10 clustering solutions, each of them for a different value of the resolution parameter γ. The following values of γ were used: 0.00001, 0.00002, 0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, and 0.01.\n",
    "\n",
    "# ahlgren 2020:\n",
    "\n",
    "Using different values of the resolution parameter γ (0.000001, 0.000002, 0.000005, 0.00001, 0.00002, 0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002), we obtain 11 clustering solutions for each relatedness measure. Compared to our earlier study (Ahlgren et al., 2019), we exclude the clustering solutions for the two largest resolution values used in that study (0.005 and 0.01). These clustering solutions have around 300,000 and 500,000 clusters, respectively, and most of the clusters consist of fewer than 10 publications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_params_graph_dict(params_graph_dict):\n",
    "    \"\"\"\n",
    "    Process the parameters and graph dictionary.\n",
    "\n",
    "    Args:\n",
    "        params_graph_dict (dict): A dictionary containing the parameters and corresponding graph.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the overall summary DataFrame and the explorer sheets dictionary.\n",
    "    \"\"\"\n",
    "    # Define the resolution values to iterate over\n",
    "    resolution_values = [\n",
    "        0.000001,\n",
    "        0.000002,\n",
    "        0.000005,\n",
    "        0.00001,\n",
    "        0.00002,\n",
    "        0.00005,\n",
    "        0.0001,\n",
    "        0.0002,\n",
    "        0.0005,\n",
    "        0.001,\n",
    "        0.002,\n",
    "        0.006,\n",
    "        0.005,\n",
    "        0.01,\n",
    "        0.02,\n",
    "        0.05,\n",
    "    ]\n",
    "    iterations = 20\n",
    "    overall_summary_dict = {\n",
    "        \"KNN-Params\": [],\n",
    "        \"Resolution\": [],\n",
    "        \"Nr of Clusters\": [],\n",
    "    }\n",
    "    explorer_sheets_dict = {}\n",
    "\n",
    "    # Iterate over each graph and resolution value\n",
    "    for params, G in tqdm(params_graph_dict.items(), desc=\"Processing graphs\"):\n",
    "        for resolution in resolution_values:\n",
    "            # Create partition using CPMVertexPartition algorithm\n",
    "            pc = PartitionCreator(G, df)\n",
    "            pc.create_partition_from_cmpvertexpartition(\n",
    "                n_iterations=iterations,\n",
    "                resolution_parameter=resolution,\n",
    "                verbose=False,\n",
    "                cluster_column_name=f\"cluster_{params}_res{resolution}\",\n",
    "            )\n",
    "\n",
    "            ## Explore communities using CommunityExplorer\n",
    "            fe = FullExplorer(\n",
    "                df=pc.df,\n",
    "                cluster_column=f\"cluster_{params}_res{resolution}\",\n",
    "                params=params,\n",
    "                resolution=resolution,\n",
    "            )\n",
    "            single_summary_dict = fe.summary_dict_creator()\n",
    "            for k, v in single_summary_dict.items():\n",
    "                overall_summary_dict[k].append(v)\n",
    "            params_df = fe.explorer_sheets_creator()\n",
    "            explorer_sheets_dict[f\"{params}_res{resolution}\"] = params_df\n",
    "\n",
    "    # Create a DataFrame with the graph parameters and the number of clusters\n",
    "    overall_summary_df = pd.DataFrame(overall_summary_dict).sort_values(\n",
    "        by=\"Nr of Clusters\", ascending=False\n",
    "    )\n",
    "    return overall_summary_df, explorer_sheets_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Partitions and Resolution Explorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing graphs: 100%|██████████| 8/8 [1:09:12<00:00, 519.06s/it]\n"
     ]
    }
   ],
   "source": [
    "overall_summary_df, explorer_sheets_dict = process_params_graph_dict(params_graph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save limited_df_graph_cluster_params as first sheet\n",
    "filepath = (\n",
    "    \"../output/tables/cluster-explorer/01-FullParameterFinder_ClusterExplorer.xlsx\"\n",
    ")\n",
    "\n",
    "NetworkAnalyzerUtils().params_excel_saver_with_hyperlinks(\n",
    "    filepath, overall_summary_df, explorer_sheets_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restrict min and max of clusters\n",
    "\n",
    "min 50 and max 500 clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full partitions_explorer_dict: 128\n",
      "filtered partitions_explorer_dict: 36\n"
     ]
    }
   ],
   "source": [
    "filtered_overall_summary_df = overall_summary_df[\n",
    "    (overall_summary_df[\"Nr of Clusters\"] > 50)\n",
    "    & (overall_summary_df[\"Nr of Clusters\"] < 500)\n",
    "]\n",
    "\n",
    "params_to_keep = (\n",
    "    filtered_overall_summary_df[\"KNN-Params\"]\n",
    "    + \"_res\"\n",
    "    + filtered_overall_summary_df[\"Resolution\"].astype(str)\n",
    ")\n",
    "\n",
    "filtered_explorer_sheets_dict = {k: explorer_sheets_dict[k] for k in params_to_keep}\n",
    "\n",
    "\n",
    "print(f\"Full partitions_explorer_dict: {len(overall_summary_df)}\")\n",
    "print(f\"filtered partitions_explorer_dict: {len(filtered_overall_summary_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save limited_df_graph_cluster_params as first sheet\n",
    "filepath = (\n",
    "    \"../output/tables/cluster-explorer/02-FilteredParameterFinder_ClusterExplorer.xlsx\"\n",
    ")\n",
    "\n",
    "NetworkAnalyzerUtils().params_excel_saver_with_hyperlinks(\n",
    "    filepath, filtered_overall_summary_df, filtered_explorer_sheets_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit Parameter Space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Criteria:\n",
    "\n",
    "1. premature ejaculation and sexual dysfunction be distinct clusters\n",
    "2. Mixing of anxiety and panic disorder?\n",
    "3. Diabetic Mice VS Body Weight and Diabetes VS depression treatment in diabetics?\n",
    "4. Use alpha = 0.3 (look at alpha0.5_k10_res0.002); topics: 50, 133 -> very mixed words, little coherence.Not the case for alpha0.3_k10_res0.002 (here, sexual dys and premature ejac single topic though)\n",
    "5. Zimeldine (1 or two topics)\n",
    "6. Pregnancy related topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepornot = {\n",
    "    # all k=5 have many many clusters of single papers\n",
    "    \"alpha0.5_k5_res1e-05\": 0,\n",
    "    \"alpha0.5_k5_res2e-05\": 0,\n",
    "    \"alpha0.5_k5_res0.0001\": 0,\n",
    "    \"alpha0.5_k5_res0.0002\": 0,\n",
    "    \"alpha0.5_k5_res0.001\": 0,\n",
    "    \"alpha0.5_k5_res0.002\": 0,\n",
    "    \"alpha0.5_k5_res0.006\": 0,\n",
    "    \"alpha0.5_k5_res0.01\": 0,\n",
    "    \"alpha0.5_k5_res0.02\": 0,\n",
    "    # K10\n",
    "    \"alpha0.5_k10_res1e-05\": 0,\n",
    "    \"alpha0.5_k10_res2e-05\": 0,\n",
    "    \"alpha0.5_k10_res0.0001\": 0,\n",
    "    \"alpha0.5_k10_res0.0002\": 0,\n",
    "    \"alpha0.5_k10_res0.001\": 0,  # premature ejaculation and sexual dysfunction and one topic\n",
    "    \"alpha0.5_k10_res0.002\": 0,  # weight and sex are one topic, same problem as above\n",
    "    \"alpha0.5_k10_res0.006\": 0,\n",
    "    \"alpha0.5_k10_res0.01\": 0,\n",
    "    \"alpha0.5_k10_res0.02\": 0,\n",
    "    # K15\n",
    "    \"alpha0.5_k15_res1e-05\": 0,\n",
    "    \"alpha0.5_k15_res2e-05\": 0,\n",
    "    \"alpha0.5_k15_res0.0001\": 0,\n",
    "    \"alpha0.5_k15_res0.0002\": 0,\n",
    "    \"alpha0.5_k15_res0.001\": 0,  # no bad, very broad and inclusive topics\n",
    "    \"alpha0.5_k15_res0.002\": 0,  # gut sex dysf and premature ejaculation are one topic\n",
    "    \"alpha0.5_k15_res0.006\": 1,  # pretty good. bit too many. too many pregnancy clusters, for example\n",
    "    \"alpha0.5_k15_res0.01\": 0,\n",
    "    \"alpha0.5_k15_res0.02\": 0,\n",
    "    # K20\n",
    "    \"alpha0.5_k20_res1e-05\": 0,\n",
    "    \"alpha0.5_k20_res2e-05\": 0,\n",
    "    \"alpha0.5_k20_res0.0001\": 0,\n",
    "    \"alpha0.5_k20_res0.0002\": 0,\n",
    "    \"alpha0.5_k20_res0.001\": 0,\n",
    "    \"alpha0.5_k20_res0.002\": 0,\n",
    "    \"alpha0.5_k20_res0.006\": 1,\n",
    "    \"alpha0.5_k20_res0.01\": 1,  # MANY MANT HT TOPCIS (something betwe 002 and 01 needed.)\n",
    "    \"alpha0.5_k20_res0.02\": 0,\n",
    "    # NEW ALPHA 0.3\n",
    "    # K5\n",
    "    \"alpha0.3_k5_res1e-05\": 0,\n",
    "    \"alpha0.3_k5_res2e-05\": 0,\n",
    "    \"alpha0.3_k5_res0.0001\": 0,\n",
    "    \"alpha0.3_k5_res0.0002\": 0,\n",
    "    \"alpha0.3_k5_res0.001\": 0,\n",
    "    \"alpha0.3_k5_res0.002\": 0,\n",
    "    \"alpha0.3_k5_res0.006\": 0,\n",
    "    \"alpha0.3_k5_res0.01\": 0,\n",
    "    \"alpha0.3_k5_res0.02\": 0,\n",
    "    # K10\n",
    "    \"alpha0.3_k10_res1e-05\": 0,\n",
    "    \"alpha0.3_k10_res2e-05\": 0,\n",
    "    \"alpha0.3_k10_res0.0001\": 0,\n",
    "    \"alpha0.3_k10_res0.0002\": 0,\n",
    "    \"alpha0.3_k10_res0.001\": 0,  # premature ejaculation and sexual dysfunction and one topic; anxiety + panic too\n",
    "    \"alpha0.3_k10_res0.002\": 1,  # gut\n",
    "    \"alpha0.3_k10_res0.006\": 1,\n",
    "    \"alpha0.3_k10_res0.01\": 0,  # good illustration for granularity using 'suicide' (500 topics)\n",
    "    \"alpha0.3_k10_res0.02\": 0,\n",
    "    # K15\n",
    "    \"alpha0.3_k15_res1e-05\": 0,\n",
    "    \"alpha0.3_k15_res2e-05\": 0,\n",
    "    \"alpha0.3_k15_res0.0001\": 0,\n",
    "    \"alpha0.3_k15_res0.0002\": 0,\n",
    "    \"alpha0.3_k15_res0.001\": 0,  # no bad, very broad and inclusive topics\n",
    "    \"alpha0.3_k15_res0.002\": 0,  # gut sex dysf and premature ejaculation are one topic\n",
    "    \"alpha0.3_k15_res0.006\": 1,  # pretty good. bit too many. too many pregnancy clusters, for example\n",
    "    \"alpha0.3_k15_res0.01\": 1,  # good illustration for granularity using 'suicide' (500 topics)\n",
    "    \"alpha0.3_k15_res0.02\": 0,\n",
    "    # K 20\n",
    "    \"alpha0.3_k20_res1e-05\": 0,\n",
    "    \"alpha0.3_k20_res2e-05\": 0,\n",
    "    \"alpha0.3_k20_res0.0001\": 0,\n",
    "    \"alpha0.3_k20_res0.0002\": 0,\n",
    "    \"alpha0.3_k20_res0.001\": 0,\n",
    "    \"alpha0.3_k20_res0.002\": 0,  # too few?\n",
    "    \"alpha0.3_k20_res0.006\": 1,  # best ?\n",
    "    \"alpha0.3_k20_res0.01\": 1,  # too many? aber gut.\n",
    "    \"alpha0.3_k20_res0.02\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save limit param space to excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/zjbwrdgj0bg9zyhx3l7134mm0000gn/T/ipykernel_22532/687705255.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_overall_summary_df[\"keep\"] = filtered_overall_summary_df.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of soltions initially: 128\n",
      "Nr of solutions after filtering: 36\n",
      "Nr of solutions after double filtering: 9\n"
     ]
    }
   ],
   "source": [
    "# Filter explorer_sheets_dict based on keepornot values\n",
    "double_filtered_explorer_sheets_dict = {\n",
    "    k: v for k, v in filtered_explorer_sheets_dict.items() if keepornot.get(k, 0) == 1\n",
    "}\n",
    "\n",
    "# Create a 'drop' flag directly in the filtering step to avoid extra column manipulation\n",
    "filtered_overall_summary_df[\"keep\"] = filtered_overall_summary_df.apply(\n",
    "    lambda row: keepornot.get(f\"{row['KNN-Params']}_res{row['Resolution']}\", 0), axis=1\n",
    ")\n",
    "double_filtered_overall_summary_df = filtered_overall_summary_df[\n",
    "    filtered_overall_summary_df[\"keep\"] == 1\n",
    "].drop(columns=[\"keep\"])\n",
    "\n",
    "# Save filtered data and sheets\n",
    "filepath = (\n",
    "    \"../output/tables/cluster-explorer/03-LimitedParameterFinder_ClusterExplorer.xlsx\"\n",
    ")\n",
    "\n",
    "# Assuming NetworkAnalyzerUtils() and its method are properly defined\n",
    "# and can handle saving a DataFrame with hyperlinks\n",
    "NetworkAnalyzerUtils().params_excel_saver_with_hyperlinks(\n",
    "    filepath, double_filtered_overall_summary_df, double_filtered_explorer_sheets_dict\n",
    ")\n",
    "\n",
    "print(f\"Nr of soltions initially: {len(overall_summary_df)}\")\n",
    "print(f\"Nr of solutions after filtering: {len(filtered_overall_summary_df)}\")\n",
    "print(\n",
    "    f\"Nr of solutions after double filtering: {len(double_filtered_overall_summary_df)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at individual solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: alpha0.3_k15_res0.01; Nr of clusters: 379\n",
      "##################################################\n",
      "Params: alpha0.5_k20_res0.01; Nr of clusters: 375\n",
      "##################################################\n",
      "Params: alpha0.3_k10_res0.006; Nr of clusters: 362\n",
      "##################################################\n",
      "Params: alpha0.3_k20_res0.01; Nr of clusters: 299\n",
      "##################################################\n",
      "Params: alpha0.5_k15_res0.006; Nr of clusters: 298\n",
      "##################################################\n",
      "Params: alpha0.3_k15_res0.006; Nr of clusters: 241\n",
      "##################################################\n",
      "Params: alpha0.5_k20_res0.006; Nr of clusters: 231\n",
      "##################################################\n",
      "Params: alpha0.3_k20_res0.006; Nr of clusters: 190\n",
      "##################################################\n",
      "Params: alpha0.3_k10_res0.002; Nr of clusters: 155\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "quality_values = []\n",
    "nr_clusters = []\n",
    "partitions = []\n",
    "graph_summary_df_dict = {}\n",
    "\n",
    "for params in double_filtered_explorer_sheets_dict.keys():\n",
    "    G = params_graph_dict[params.split(\"_res\")[0]]\n",
    "    resolution = pd.to_numeric(params.split(\"res\")[1])\n",
    "    pc = PartitionCreator(G, df)\n",
    "    pc.create_partition_from_cmpvertexpartition(\n",
    "        n_iterations=iterations,\n",
    "        resolution_parameter=resolution,\n",
    "        verbose=False,\n",
    "        cluster_column_name=f\"cluster_{params}\",\n",
    "        centrality_column_name=f\"centrality_{params}\",\n",
    "    )\n",
    "    #############################\n",
    "    #############################\n",
    "    ce = CommunityExplorer(\n",
    "        df=pc.df,\n",
    "        cluster_column=f\"cluster_{params}\",\n",
    "        sort_column=f\"centrality_{params}\",\n",
    "        nr_tiles=15,\n",
    "    )\n",
    "    ce.create_full_explorer()\n",
    "    df_summary, cluster_titles_sheets_dict = ce.full_return()\n",
    "\n",
    "    # save to excel\n",
    "    filepath = f\"../output/tables/cluster-explorer/SingleSolExplorer_{params}.xlsx\"\n",
    "    NetworkAnalyzerUtils().clusters_excel_saver_with_hyperlinks(\n",
    "        filepath, df_summary, cluster_titles_sheets_dict\n",
    "    )\n",
    "    print(f\"Params: {params}; Nr of clusters: {len(pc.partition.sizes())}\")\n",
    "    print(\"#\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore based on Decision Criteria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Criteria:\n",
    "\n",
    "1. premature ejaculation and sexual dysfunction be distinct clusters\n",
    "2. Mixing of anxiety and panic disorder?\n",
    "3. Diabetic Mice VS Body Weight and Diabetes VS depression treatment in diabetics?\n",
    "4. Use alpha = 0.3 (look at alpha0.5_k10_res0.002); topics: 50, 133 -> very mixed words, little coherence.Not the case for alpha0.3_k10_res0.002 (here, sexual dys and premature ejac single topic though)\n",
    "5. Zimeldine (1 or two topics)\n",
    "6. Pregnancy related topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: EJAC DYSF\n",
      "\n",
      "No hits found for Parameters: \n",
      "set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis_data_dict = graph_summary_df_dict\n",
    "print_n_random_titles = 5\n",
    "word1 = \"ejac\"\n",
    "word2 = \"dysf\"\n",
    "print_word_string = True\n",
    "save_to_file = True\n",
    "file_name = f\"../output/tables/cluster-explorer/ParamsTestTxt/cluster_analysis_output_{word1}.txt\"\n",
    "\n",
    "NetworkAnalyzerUtils().cluster_coherence_analyzer_in_txt(\n",
    "    analysis_data_dict=analysis_data_dict,\n",
    "    print_n_random_titles=print_n_random_titles,\n",
    "    word1=word1,\n",
    "    word2=word2,\n",
    "    print_word_string=print_word_string,\n",
    "    save_to_file=save_to_file,\n",
    "    file_name=file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: PREG \n",
      "\n",
      "No hits found for Parameters: \n",
      "set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis_data_dict = graph_summary_df_dict\n",
    "print_n_random_titles = 10\n",
    "word1 = \"preg\"\n",
    "word2 = \"\"\n",
    "print_word_string = True\n",
    "save_to_file = True\n",
    "file_name = f\"../output/tables/cluster-explorer/ParamsTestTxt/cluster_analysis_output_{word1}.txt\"\n",
    "\n",
    "NetworkAnalyzerUtils().cluster_coherence_analyzer_in_txt(\n",
    "    analysis_data_dict=analysis_data_dict,\n",
    "    print_n_random_titles=print_n_random_titles,\n",
    "    word1=word1,\n",
    "    word2=word2,\n",
    "    print_word_string=print_word_string,\n",
    "    save_to_file=save_to_file,\n",
    "    file_name=file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUICIDE\n",
    "params_scoring_suicide = {\n",
    "    \"alpha0.3_k10_res0.002\": \"1\",\n",
    "    \"alpha0.3_k10_res0.006\": \"0 (too many suicide clusters, very inconsistent)\",\n",
    "    \"alpha0.3_k15_res0.006\": \"1 (very clear singel suicide cluster)\",\n",
    "    \"alpha0.3_k15_res0.01\": \"0 (conflates overdoses and suicide; too many clusters on it; 1 clear)\",\n",
    "    \"alpha0.3_k20_res0.006\": \"1 (one good suicide, one unrelated)\",\n",
    "    \"alpha0.3_k20_res0.01\": \"1 (two clear suicide clusters; should be same)\",\n",
    "    \"alpha0.3_k20_res0.02\": \"0 (akathisia cluster; boderline cluster; no clear bigger suicide cluster)\",\n",
    "    \"alpha0.5_k15_res0.006\": \"0 no good suicide cluster\",\n",
    "    \"alpha0.5_k20_res0.006\": \"1 (one clear suicide cluster, one overdose/drug)\",\n",
    "    \"alpha0.5_k20_res0.01\": \"0 first biggest cluster is very bad\",\n",
    "}\n",
    "# pregnancy\n",
    "params_scoring_pregnancy = {\n",
    "    \"alpha0.3_k10_res0.002\": \"1 (single cluster, coherent)\",\n",
    "    \"alpha0.3_k10_res0.006\": \"0 (pharma pregnancy, narcolepsy, lectation, neonatal, TOO MANY)\",\n",
    "    \"alpha0.3_k15_res0.006\": \"0 (4 clusters, not one clear preggo cluster)\",\n",
    "    \"alpha0.3_k15_res0.01\": \"0 (1 big trash topic; pretty bad)\",\n",
    "    \"alpha0.3_k20_res0.006\": \"0 (one small preggo cluster)\",\n",
    "    \"alpha0.3_k20_res0.01\": \"1 (bit too split up, but sensical, [pretty good])\",\n",
    "    \"alpha0.3_k20_res0.02\": \"0 (creation seems wieird.)\",\n",
    "    \"alpha0.5_k15_res0.006\": \"0 (biggest cluster sucks. creation went weird)\",\n",
    "    \"alpha0.5_k20_res0.006\": \"0 (first biggest cluster is weird)\",\n",
    "    \"alpha0.5_k20_res0.01\": \"0 (small cluster, seem sensical tho)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last_selection_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: alpha0.5_k20_res0.006; Nr of clusters: 229\n",
      "##################################################\n",
      "Params: alpha0.3_k10_res0.002; Nr of clusters: 154\n",
      "##################################################\n",
      "Params: alpha0.3_k20_res0.01; Nr of clusters: 299\n",
      "##################################################\n",
      "Params: alpha0.3_k20_res0.006; Nr of clusters: 190\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "iterations = 50\n",
    "graph_summary_df_dict = {}\n",
    "\n",
    "last_selection_params = [\n",
    "    \"alpha0.5_k20_res0.006\",\n",
    "    \"alpha0.3_k10_res0.002\",\n",
    "    \"alpha0.3_k20_res0.01\",\n",
    "    \"alpha0.3_k20_res0.006\",\n",
    "]\n",
    "\n",
    "\n",
    "for params in last_selection_params:\n",
    "    G = params_graph_dict[params.split(\"_res\")[0]]\n",
    "    resolution = pd.to_numeric(params.split(\"res\")[1])\n",
    "    column_name = f\"cluster_{params}\"\n",
    "    pc = PartitionCreator(G, df)\n",
    "    pc.create_partition_from_cmpvertexpartition(\n",
    "        n_iterations=iterations,\n",
    "        resolution_parameter=resolution,\n",
    "        verbose=False,\n",
    "        cluster_column_name=column_name,\n",
    "        centrality_column_name=f\"centrality_{params}\",\n",
    "    )\n",
    "    #############################\n",
    "    #############################\n",
    "    ce = CommunityExplorer(\n",
    "        df=pc.df,\n",
    "        cluster_column=f\"cluster_{params}\",\n",
    "        sort_column=f\"centrality_{params}\",\n",
    "        nr_tiles=15,\n",
    "    )\n",
    "    ce.create_full_explorer()\n",
    "    df_summary, cluster_titles_sheets_dict = ce.full_return()\n",
    "\n",
    "    # save to excel\n",
    "    filepath = (\n",
    "        f\"../output/tables/cluster-explorer/FinalSelect/SingleSolExplorer_{params}.xlsx\"\n",
    "    )\n",
    "    NetworkAnalyzerUtils().clusters_excel_saver_with_hyperlinks(\n",
    "        filepath, df_summary, cluster_titles_sheets_dict\n",
    "    )\n",
    "    print(f\"Params: {params}; Nr of clusters: {len(pc.partition.sizes())}\")\n",
    "    print(\"#\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL SELECTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha0.3_k10_res0.002; Nr of clusters: 154\n",
    "\n",
    "1. \"alpha0.3_k10_res0.002\": \"1 - suicide\",\n",
    "2. \"alpha0.3_k10_res0.002\": \"1 (single cluster, coherent) - pregnancy\",\n",
    "\n",
    "##### EVAL TOP 5 CLSUTERS\n",
    "\n",
    "0. Very clear memory clsuter; pharma and not; 1199 pubs\n",
    "1. Receptor binding; brain; guess its good\n",
    "2. QTC prolongation; cardiovascular; good\n",
    "3. astrocytes; pharma; good\n",
    "4. Binding; platelets check difference to 2; (kinda good)\n",
    "5. Pulmonary hypertension; bluthochdruck; (kinda good)\n",
    "\n",
    "## alpha0.3_k20_res0.01; Nr of clusters: 299\n",
    "\n",
    "1. \"alpha0.3_k20_res0.01\": \"1 (two clear suicide clusters; should be same) - suicide\",\n",
    "2. \"alpha0.3_k20_res0.01\": \"1 (bit too split up, but sensical, [pretty good]) - pregnancy\",\n",
    "3. Good elderly cluster\n",
    "4. Good Fall cluster (in elderly)\n",
    "5. Good single seasonal cluster\n",
    "6. good withdrawal/discontinuation cluster\n",
    "\n",
    "##### EVAL TOP 5 CLSUTERS\n",
    "\n",
    "0. Learning; Memory;hippocampus; fear (avoidance learning); Ok Good\n",
    "1. Prolactin; Pharmacology; ok Good\n",
    "2. Cardiac,arrythmia, qt prolongation (very good)\n",
    "3. pregnancy; placenta, a preggo pharma cluster\n",
    "4. SSRI mechanisms; Pharmacology; Depression treatment\n",
    "5. Pharmacology; 5-HT; receptor; SERT(serotonintransporter),\n",
    "\n",
    "## alpha0.3_k20_res0.006; Nr of clusters: 190\n",
    "\n",
    "1. \"alpha0.3_k20_res0.01\": \"1 (two clear suicide clusters; should be same)- suicide\",\n",
    "2. \"alpha0.3_k20_res0.01\": \"1 (bit too split up, but sensical, [pretty good])- pregnancy\",\n",
    "\n",
    "##### EVAL TOP 5 CLSUTERS\n",
    "\n",
    "0. Memory; Cognition; Learning;\n",
    "1. Receptor; Animal studies; IN vivo; autoreceptor;\n",
    "2. cardiac; qt prolongation\n",
    "3. astrocytes; pharmaco\n",
    "4. pharmacological ssri properties; 5 ht; receptors\n",
    "5. vascular; rat; unclear;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/04-embeddings/df_with_specter2_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: alpha0.5_k20_res0.006; Nr of clusters: 229\n",
      "##################################################\n",
      "Params: alpha0.3_k10_res0.002; Nr of clusters: 154\n",
      "##################################################\n",
      "Params: alpha0.3_k20_res0.01; Nr of clusters: 299\n",
      "##################################################\n",
      "Params: alpha0.3_k20_res0.006; Nr of clusters: 190\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code chunk performs community detection on a graph using different parameter configurations. It iterates over a list of parameter strings, extracts the necessary values from each string, and creates a partition based on the graph and the extracted parameters. It then drops unnecessary columns from the partition's dataframe, adds cluster labels to the graph, and saves the resulting graph as a GraphML file. Finally, it saves the partition's dataframe as a pickle file.\n",
    "\"\"\"\n",
    "\n",
    "final_params_list = [\n",
    "    \"alpha0.5_k20_res0.006\",\n",
    "    \"alpha0.3_k10_res0.002\",\n",
    "    \"alpha0.3_k20_res0.01\",\n",
    "    \"alpha0.3_k20_res0.006\",\n",
    "]\n",
    "iterations = 50\n",
    "\n",
    "\n",
    "for params in final_params_list:\n",
    "    G = params_graph_dict[params.split(\"_res\")[0]]\n",
    "    resolution = pd.to_numeric(params.split(\"res\")[1])\n",
    "    pc = PartitionCreator(G, df)\n",
    "    pc.create_partition_from_cmpvertexpartition(\n",
    "        n_iterations=iterations,\n",
    "        resolution_parameter=resolution,\n",
    "        verbose=False,\n",
    "        cluster_column_name=f\"cluster_{params}\",\n",
    "        centrality_column_name=f\"centrality_{params}\",\n",
    "    )\n",
    "\n",
    "    # save graphml\n",
    "    pc.G.write_graphml(f\"../data/07-clustered-graphs/{params}.graphml\")\n",
    "    path = f\"../data/06-clustered-df/{params}.pkl\"\n",
    "    pc.df.to_pickle(path)\n",
    "    #############################\n",
    "    #############################\n",
    "    ce = CommunityExplorer(\n",
    "        df=pc.df,\n",
    "        cluster_column=f\"cluster_{params}\",\n",
    "        sort_column=f\"centrality_{params}\",\n",
    "        nr_tiles=15,\n",
    "    )\n",
    "    ce.create_full_explorer()\n",
    "    df_summary, cluster_titles_sheets_dict = ce.full_return()\n",
    "\n",
    "    # save to excel\n",
    "    filepath = (\n",
    "        f\"../output/tables/cluster-explorer/FinalSelect/SingleSolExplorer_{params}.xlsx\"\n",
    "    )\n",
    "    NetworkAnalyzerUtils().clusters_excel_saver_with_hyperlinks(\n",
    "        filepath, df_summary, cluster_titles_sheets_dict\n",
    "    )\n",
    "    print(f\"Params: {params}; Nr of clusters: {len(pc.partition.sizes())}\")\n",
    "    print(\"#\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving igraph to pajek format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a new NetworkX graph\n",
    "G_nx = nx.Graph()\n",
    "\n",
    "# Add nodes to the NetworkX graph\n",
    "for vertex in G.vs:\n",
    "    G_nx.add_node(vertex.index, **vertex.attributes())\n",
    "\n",
    "# Add edges to the NetworkX graph\n",
    "for edge in G.es:\n",
    "    G_nx.add_edge(edge.source, edge.target, **edge.attributes())\n",
    "\n",
    "\n",
    "# Remove all attributes except 'eid' from each node\n",
    "for node, data in G_nx.nodes(data=True):\n",
    "    keys_to_remove = [\n",
    "        k for k in data if k not in [\"unique_auth_year\", \"eid\"]\n",
    "    ]  # Collect keys to remove\n",
    "    for k in keys_to_remove:\n",
    "        data.pop(k, None)  # Remove the key\n",
    "\n",
    "# Now, convert all remaining node attributes to strings\n",
    "for node, data in G_nx.nodes(data=True):\n",
    "    for k, v in data.items():\n",
    "        data[k] = str(v)  # Convert attribute to string\n",
    "\n",
    "# Gpajek is now modified with only 'eid' attributes retained and converted to string\n",
    "# You can now save Gpajek to the Pajek format\n",
    "nx.write_pajek(G_nx, \"../data/07-clustered-graphs/alpha0.3_k20_res0.006.net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW WE TAKE A CLOSER LOOK AT THE BEST PARAMS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
