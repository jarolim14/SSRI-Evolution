{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Reference Data from Scopus API\n",
    "\n",
    "Resources:\n",
    "\n",
    "- [Scopus Abstract Retrieval Views](https://dev.elsevier.com/sc_abstract_retrieval_views.html)\n",
    "- [Scopus Retrieval API](https://dev.elsevier.com/documentation/AbstractRetrievalAPI.wadl)\n",
    "- [Interactive Scopus API](https://dev.elsevier.com/scopus.html)\n",
    "- [API Settings (rate limits)](https://dev.elsevier.com/api_key_settings.html)\n",
    "- Remember Logging In to Cisco VPN!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the `ScopusReferenceFetcher` to retrieve references for a list of Scopus IDs.\n",
    "\n",
    "Key features:\n",
    "1. Automatic API key rotation when rate limits are hit\n",
    "2. Saves progress after every 500 requests\n",
    "3. Resumes from last saved state if interrupted\n",
    "4. Comprehensive logging of progress and errors\n",
    "5. Handles multiple API keys with different rate limits\n",
    "\n",
    "The process:\n",
    "1. Loads previously processed data to avoid duplicates\n",
    "2. Filters the input dataframe to only unprocessed articles\n",
    "3. Processes articles in batches, rotating API keys as needed\n",
    "4. Saves progress after each batch\n",
    "5. Logs all operations and errors for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from math import ceil\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.data.ScopusReferenceFetcher import ScopusReferenceFetcher\n",
    "from src.data.ScopusReferenceFetcherUtils import (\n",
    "    ScopusRefFetcherPrep,\n",
    "    ScopusRefFetcherProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment:  rate limits are 40,000 per week for api_key_A and 10,000 for every other key\n",
      "Available API keys: api_key_A, api_key_B, api_key_deb, api_key_haoxin\n",
      "Total articles to process: 38961\n"
     ]
    }
   ],
   "source": [
    "# Get all API keys\n",
    "api_keys = ScopusRefFetcherPrep.get_api_keys()\n",
    "print(f\"Available API keys: {', '.join(api_keys.keys())}\")\n",
    "\n",
    "# Setup logging\n",
    "log_directory = \"../data/01-raw/references\"\n",
    "ScopusRefFetcherProcessor.setup_logging(log_directory, log_level=logging.INFO)\n",
    "\n",
    "\n",
    "# load data\n",
    "df_path = \"../data/02-clean/articles/scopus_cleaned_20250326_081230.pkl\"\n",
    "df = pd.read_pickle(df_path)\n",
    "print(f\"Total articles to process: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Previously Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m refs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/01-raw/references\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# create this \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m eids, max_batch \u001b[38;5;241m=\u001b[39m prepper\u001b[38;5;241m.\u001b[39mload_fetched_reference_data(refs_path)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Filter dataframe to unprocessed articles\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m prepper\u001b[38;5;241m.\u001b[39mload_and_filter_articles(df_path, eids)\n",
      "File \u001b[0;32m~/Projects/Study-1-Bibliometrics/src/data/ScopusReferenceFetcherUtils.py:38\u001b[0m, in \u001b[0;36mScopusRefFetcherPrep.load_fetched_reference_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Extract batch numbers and find the maximum\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m max_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files with batch numbers up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m eids \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# Initialize prepper\n",
    "prepper = ScopusRefFetcherPrep()\n",
    "\n",
    "# Load previously processed data\n",
    "refs_path = \"../data/01-raw/references\"\n",
    "# create this \n",
    "eids, max_batch = prepper.load_fetched_reference_data(refs_path)\n",
    "\n",
    "# Filter dataframe to unprocessed articles\n",
    "df_filtered = prepper.load_and_filter_articles(df_path, eids)\n",
    "\n",
    "print(f\"Previously processed articles: {len(eids)}\")\n",
    "print(f\"Remaining articles to process: {len(df_filtered)}\")\n",
    "print(f\"Last processed batch: {max_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = ScopusRefFetcherProcessor()\n",
    "\n",
    "# Process batches with automatic API key rotation\n",
    "processor.process_scopus_batches(\n",
    "    api_keys=api_keys,\n",
    "    df_to_fetch=df_filtered,\n",
    "    data_path=\"../data/01-raw/references\",\n",
    "    last_processed_batch=max_batch,\n",
    "    batch_size=500,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibliometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
