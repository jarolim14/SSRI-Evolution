{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.nlp.EmbeddingCreator import EmbeddingCreator\n",
    "from src.nlp.TextProcessor import TextProcessor\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_pickle(\"../data/03-interim/merged/merged_data_with_refs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class PaperEmbeddingProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        model_name,\n",
    "        adapter_name,\n",
    "        save_dir,\n",
    "        batch_size=32,\n",
    "        chunk_size=100,  # Manageable chunk size for memory\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.df[\"title\"] = self.df[\"title\"].fillna(\"\")\n",
    "        self.df[\"abstract\"] = self.df[\"abstract\"].fillna(\"\")\n",
    "        self.chunk_size = chunk_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoAdapterModel.from_pretrained(model_name)\n",
    "        self.model.load_adapter(\n",
    "            \"allenai/specter2\",\n",
    "            source=\"hf\",\n",
    "            load_as=adapter_name,\n",
    "            set_active=True,\n",
    "        )\n",
    "        self.save_dir = save_dir\n",
    "        self.batch_size = batch_size\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        text_batch = [\n",
    "            d[\"title\"] + self.tokenizer.sep_token + (d.get(\"abstract\") or \"\")\n",
    "            for d in batch\n",
    "        ]\n",
    "        inputs = self.tokenizer(\n",
    "            text_batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**inputs)\n",
    "        embeddings = output.last_hidden_state[:, 0, :]\n",
    "        return embeddings.cpu()\n",
    "\n",
    "    def process_papers(self):\n",
    "        all_embeddings = []\n",
    "        total_records = len(self.df)\n",
    "\n",
    "        for start_idx in tqdm(range(0, total_records, self.chunk_size)):\n",
    "            end_idx = min(start_idx + self.chunk_size, total_records)\n",
    "            batch_embeddings = []\n",
    "\n",
    "            for batch_start in range(start_idx, end_idx, self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, end_idx)\n",
    "                batch = self.df.iloc[batch_start:batch_end].to_dict(orient=\"records\")\n",
    "\n",
    "                if batch:  # Check if the batch is not empty\n",
    "                    embeddings = self.process_batch(batch)\n",
    "                    batch_embeddings.append(embeddings)\n",
    "\n",
    "                    # Clear memory\n",
    "                    del embeddings\n",
    "                    gc.collect()\n",
    "\n",
    "            # Save and accumulate batch embeddings\n",
    "            if batch_embeddings:  # Check if there are embeddings to concatenate\n",
    "                batch_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "                batch_file = os.path.join(\n",
    "                    self.save_dir, f\"embeddings_chunk_{start_idx//self.chunk_size}.pt\"\n",
    "                )\n",
    "                torch.save(batch_embeddings, batch_file)\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "\n",
    "        total_embeddings = (\n",
    "            torch.cat(all_embeddings, dim=0) if all_embeddings else torch.tensor([])\n",
    "        )\n",
    "        torch.save(total_embeddings, os.path.join(self.save_dir, \"total_embeddings.pt\"))\n",
    "        return total_embeddings\n",
    "\n",
    "    def save_embeddings_with_data(\n",
    "        self, embeddings, file_name=\"df_with_specter2_embeddings.pkl\"\n",
    "    ):\n",
    "        self.df[\"specter2_embeddings\"] = list(embeddings.numpy())\n",
    "        self.df.to_pickle(os.path.join(self.save_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85084e9ed6e478d946a6e64de8da89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [7:09:10<00:00, 1716.70s/it]  \n"
     ]
    }
   ],
   "source": [
    "processor = PaperEmbeddingProcessor(\n",
    "    df=df,\n",
    "    model_name=\"allenai/specter2_base\",\n",
    "    adapter_name=\"specter2\",\n",
    "    save_dir=\"../data/03-interim/specter-embeddings\",\n",
    "    batch_size=32,\n",
    "    chunk_size=2500,\n",
    ")\n",
    "total_embeddings = processor.process_papers()\n",
    "processor.save_embeddings_with_data(total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_cosine_similarity_matrix(df, embeddings_column):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity matrix for the embeddings in a DataFrame column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the embeddings.\n",
    "    embeddings_column (str): The name of the column containing embeddings.\n",
    "    sample_size (int, optional): The number of samples to consider. Defaults to 1000.\n",
    "    random_state (int, optional): The seed for random sampling. Defaults to 1887.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: A cosine similarity matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the embeddings column to a 2D NumPy array\n",
    "    embeddings = np.stack(df[embeddings_column])\n",
    "\n",
    "    # Check for NaN or infinite values and handle them\n",
    "    embeddings = np.nan_to_num(embeddings)\n",
    "\n",
    "    # Normalize the embeddings to unit length\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embeddings / norms\n",
    "\n",
    "    # Compute the cosine similarity matrix\n",
    "    cosine_similarity_matrix = np.dot(normalized_embeddings, normalized_embeddings.T)\n",
    "\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# cosine_similarity_matrix = compute_cosine_similarity_matrix(df_connected, \"embeddings_allmpnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix = compute_cosine_similarity_matrix(\n",
    "    processor.df, \"specter2_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831603</td>\n",
       "      <td>0.819804</td>\n",
       "      <td>0.913932</td>\n",
       "      <td>0.867501</td>\n",
       "      <td>0.840011</td>\n",
       "      <td>0.780502</td>\n",
       "      <td>0.829427</td>\n",
       "      <td>0.794649</td>\n",
       "      <td>0.850203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862365</td>\n",
       "      <td>0.880004</td>\n",
       "      <td>0.833291</td>\n",
       "      <td>0.872294</td>\n",
       "      <td>0.848742</td>\n",
       "      <td>0.845483</td>\n",
       "      <td>0.862239</td>\n",
       "      <td>0.875230</td>\n",
       "      <td>0.856609</td>\n",
       "      <td>0.887444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.831603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836459</td>\n",
       "      <td>0.849612</td>\n",
       "      <td>0.874288</td>\n",
       "      <td>0.854254</td>\n",
       "      <td>0.845415</td>\n",
       "      <td>0.865543</td>\n",
       "      <td>0.830590</td>\n",
       "      <td>0.845462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859995</td>\n",
       "      <td>0.851207</td>\n",
       "      <td>0.858259</td>\n",
       "      <td>0.851953</td>\n",
       "      <td>0.876773</td>\n",
       "      <td>0.902258</td>\n",
       "      <td>0.845239</td>\n",
       "      <td>0.893188</td>\n",
       "      <td>0.799374</td>\n",
       "      <td>0.834765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.819804</td>\n",
       "      <td>0.836459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823196</td>\n",
       "      <td>0.853579</td>\n",
       "      <td>0.843751</td>\n",
       "      <td>0.818995</td>\n",
       "      <td>0.850880</td>\n",
       "      <td>0.861743</td>\n",
       "      <td>0.829988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862416</td>\n",
       "      <td>0.855353</td>\n",
       "      <td>0.883752</td>\n",
       "      <td>0.842998</td>\n",
       "      <td>0.832910</td>\n",
       "      <td>0.865265</td>\n",
       "      <td>0.888510</td>\n",
       "      <td>0.877493</td>\n",
       "      <td>0.817226</td>\n",
       "      <td>0.815044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.913932</td>\n",
       "      <td>0.849612</td>\n",
       "      <td>0.823196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877501</td>\n",
       "      <td>0.859489</td>\n",
       "      <td>0.818500</td>\n",
       "      <td>0.821718</td>\n",
       "      <td>0.818351</td>\n",
       "      <td>0.852304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864038</td>\n",
       "      <td>0.886404</td>\n",
       "      <td>0.833654</td>\n",
       "      <td>0.877436</td>\n",
       "      <td>0.858027</td>\n",
       "      <td>0.849932</td>\n",
       "      <td>0.849155</td>\n",
       "      <td>0.878593</td>\n",
       "      <td>0.815456</td>\n",
       "      <td>0.909829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.867501</td>\n",
       "      <td>0.874288</td>\n",
       "      <td>0.853579</td>\n",
       "      <td>0.877501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867502</td>\n",
       "      <td>0.859598</td>\n",
       "      <td>0.857779</td>\n",
       "      <td>0.846112</td>\n",
       "      <td>0.879685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871438</td>\n",
       "      <td>0.902499</td>\n",
       "      <td>0.859350</td>\n",
       "      <td>0.896759</td>\n",
       "      <td>0.873450</td>\n",
       "      <td>0.891842</td>\n",
       "      <td>0.872624</td>\n",
       "      <td>0.898147</td>\n",
       "      <td>0.834942</td>\n",
       "      <td>0.880941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.845483</td>\n",
       "      <td>0.902258</td>\n",
       "      <td>0.865265</td>\n",
       "      <td>0.849932</td>\n",
       "      <td>0.891842</td>\n",
       "      <td>0.877556</td>\n",
       "      <td>0.860332</td>\n",
       "      <td>0.853012</td>\n",
       "      <td>0.847901</td>\n",
       "      <td>0.860254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862043</td>\n",
       "      <td>0.857571</td>\n",
       "      <td>0.870559</td>\n",
       "      <td>0.858161</td>\n",
       "      <td>0.880292</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886039</td>\n",
       "      <td>0.904251</td>\n",
       "      <td>0.845558</td>\n",
       "      <td>0.853190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.862239</td>\n",
       "      <td>0.845239</td>\n",
       "      <td>0.888510</td>\n",
       "      <td>0.849155</td>\n",
       "      <td>0.872624</td>\n",
       "      <td>0.845260</td>\n",
       "      <td>0.814057</td>\n",
       "      <td>0.865789</td>\n",
       "      <td>0.823734</td>\n",
       "      <td>0.858040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901945</td>\n",
       "      <td>0.855924</td>\n",
       "      <td>0.879595</td>\n",
       "      <td>0.859307</td>\n",
       "      <td>0.916865</td>\n",
       "      <td>0.886039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888929</td>\n",
       "      <td>0.827643</td>\n",
       "      <td>0.845197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.875230</td>\n",
       "      <td>0.893188</td>\n",
       "      <td>0.877493</td>\n",
       "      <td>0.878593</td>\n",
       "      <td>0.898147</td>\n",
       "      <td>0.893017</td>\n",
       "      <td>0.845420</td>\n",
       "      <td>0.871651</td>\n",
       "      <td>0.849364</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883821</td>\n",
       "      <td>0.877932</td>\n",
       "      <td>0.881745</td>\n",
       "      <td>0.897596</td>\n",
       "      <td>0.885275</td>\n",
       "      <td>0.904251</td>\n",
       "      <td>0.888929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855151</td>\n",
       "      <td>0.863556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.856609</td>\n",
       "      <td>0.799374</td>\n",
       "      <td>0.817226</td>\n",
       "      <td>0.815456</td>\n",
       "      <td>0.834942</td>\n",
       "      <td>0.842685</td>\n",
       "      <td>0.770327</td>\n",
       "      <td>0.813183</td>\n",
       "      <td>0.788695</td>\n",
       "      <td>0.826106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811351</td>\n",
       "      <td>0.825791</td>\n",
       "      <td>0.828062</td>\n",
       "      <td>0.820177</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>0.845558</td>\n",
       "      <td>0.827643</td>\n",
       "      <td>0.855151</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.887444</td>\n",
       "      <td>0.834765</td>\n",
       "      <td>0.815044</td>\n",
       "      <td>0.909829</td>\n",
       "      <td>0.880941</td>\n",
       "      <td>0.889278</td>\n",
       "      <td>0.803931</td>\n",
       "      <td>0.836358</td>\n",
       "      <td>0.825745</td>\n",
       "      <td>0.858084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849219</td>\n",
       "      <td>0.874934</td>\n",
       "      <td>0.814632</td>\n",
       "      <td>0.870306</td>\n",
       "      <td>0.831845</td>\n",
       "      <td>0.853190</td>\n",
       "      <td>0.845197</td>\n",
       "      <td>0.863556</td>\n",
       "      <td>0.820599</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    1.000000  0.831603  0.819804  0.913932  0.867501  0.840011  0.780502   \n",
       "1    0.831603  1.000000  0.836459  0.849612  0.874288  0.854254  0.845415   \n",
       "2    0.819804  0.836459  1.000000  0.823196  0.853579  0.843751  0.818995   \n",
       "3    0.913932  0.849612  0.823196  1.000000  0.877501  0.859489  0.818500   \n",
       "4    0.867501  0.874288  0.853579  0.877501  1.000000  0.867502  0.859598   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.845483  0.902258  0.865265  0.849932  0.891842  0.877556  0.860332   \n",
       "996  0.862239  0.845239  0.888510  0.849155  0.872624  0.845260  0.814057   \n",
       "997  0.875230  0.893188  0.877493  0.878593  0.898147  0.893017  0.845420   \n",
       "998  0.856609  0.799374  0.817226  0.815456  0.834942  0.842685  0.770327   \n",
       "999  0.887444  0.834765  0.815044  0.909829  0.880941  0.889278  0.803931   \n",
       "\n",
       "          7         8         9    ...       990       991       992  \\\n",
       "0    0.829427  0.794649  0.850203  ...  0.862365  0.880004  0.833291   \n",
       "1    0.865543  0.830590  0.845462  ...  0.859995  0.851207  0.858259   \n",
       "2    0.850880  0.861743  0.829988  ...  0.862416  0.855353  0.883752   \n",
       "3    0.821718  0.818351  0.852304  ...  0.864038  0.886404  0.833654   \n",
       "4    0.857779  0.846112  0.879685  ...  0.871438  0.902499  0.859350   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.853012  0.847901  0.860254  ...  0.862043  0.857571  0.870559   \n",
       "996  0.865789  0.823734  0.858040  ...  0.901945  0.855924  0.879595   \n",
       "997  0.871651  0.849364  0.877527  ...  0.883821  0.877932  0.881745   \n",
       "998  0.813183  0.788695  0.826106  ...  0.811351  0.825791  0.828062   \n",
       "999  0.836358  0.825745  0.858084  ...  0.849219  0.874934  0.814632   \n",
       "\n",
       "          993       994       995       996       997       998       999  \n",
       "0    0.872294  0.848742  0.845483  0.862239  0.875230  0.856609  0.887444  \n",
       "1    0.851953  0.876773  0.902258  0.845239  0.893188  0.799374  0.834765  \n",
       "2    0.842998  0.832910  0.865265  0.888510  0.877493  0.817226  0.815044  \n",
       "3    0.877436  0.858027  0.849932  0.849155  0.878593  0.815456  0.909829  \n",
       "4    0.896759  0.873450  0.891842  0.872624  0.898147  0.834942  0.880941  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.858161  0.880292  1.000000  0.886039  0.904251  0.845558  0.853190  \n",
       "996  0.859307  0.916865  0.886039  1.000000  0.888929  0.827643  0.845197  \n",
       "997  0.897596  0.885275  0.904251  0.888929  1.000000  0.855151  0.863556  \n",
       "998  0.820177  0.795500  0.845558  0.827643  0.855151  1.000000  0.820599  \n",
       "999  0.870306  0.831845  0.853190  0.845197  0.863556  0.820599  1.000000  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 16: Selective serotonin-reuptake inhibitor-induced movement disorders\n",
      "Second most similar paper:\n",
      "Paper 491: Movement disorders associated with the serotonin selective reuptake inhibitors\n",
      "Cosine similarity: 0.9632427096366882\n",
      "----\n",
      "Paper 46: Central nervous system active medications and risk for fractures in older women\n",
      "Second most similar paper:\n",
      "Paper 784: Use of antidepressant medications and risk of fracture in older women\n",
      "Cosine similarity: 0.9643115401268005\n",
      "----\n",
      "Paper 60: Contribution of post-secretory mechanisms to the observed pattern of histamine and 5-hydroxytryptamine secretion from peritoneal rat mast cells in response to compound 48/80\n",
      "Second most similar paper:\n",
      "Paper 529: Differential release of histamine and 5-hydroxytryptamine from rat mast cells: The contribution of amine uptake to the apparent pattern of secretion\n",
      "Cosine similarity: 0.9715805053710938\n",
      "----\n",
      "Paper 65: Achieving remission from depression with venlafaxine and venlafaxine extended release: A literature review of comparative studies with selective serotonin reuptake inhibitors\n",
      "Second most similar paper:\n",
      "Paper 189: Response and remission rates in different subpopulations with major depressive disorder administered venlafaxine, selective serotonin reuptake inhibitors, or placebo\n",
      "Cosine similarity: 0.9611566066741943\n",
      "----\n",
      "Paper 86: Depression in Parkinson's disease\n",
      "Second most similar paper:\n",
      "Paper 844: Depression in patients with Parkinson's disease\n",
      "Cosine similarity: 0.9657835364341736\n",
      "----\n",
      "Paper 157: Linezolid interaction with serotonin reuptake inhibitors: Report of two cases and incidence assessment\n",
      "Second most similar paper:\n",
      "Paper 679: Serotonin toxicity associated with the use of linezolid: A review of postmarketing data\n",
      "Cosine similarity: 0.9624748229980469\n",
      "----\n",
      "Paper 226: Demographic and clinical factors associated with different antidepressant treatments: a retrospective cohort study design in a UK psychiatric healthcare setting\n",
      "Second most similar paper:\n",
      "Paper 951: Antidepressant use and risk of suicide and attempted suicide or self harm in people aged 20 to 64: Cohort study using a primary care database\n",
      "Cosine similarity: 0.9631392359733582\n",
      "----\n",
      "Paper 235: Antidepressants\n",
      "Second most similar paper:\n",
      "Paper 890: Expanding Psychopharmacologic Treatment Options for the Depressed Medical Patient\n",
      "Cosine similarity: 0.9607509970664978\n",
      "----\n",
      "Paper 314: Psychopharmacologic treatment of pediatric major depressive disorder\n",
      "Second most similar paper:\n",
      "Paper 750: Pharmacotherapy for major depression in children and adolescents\n",
      "Cosine similarity: 0.9641734957695007\n",
      "----\n",
      "Paper 369: The antidepressant drug, sertraline, hinders bone healing and osseointegration in rats’ tibiae\n",
      "Second most similar paper:\n",
      "Paper 452: Selective serotonin re-uptake inhibitor sertraline inhibits bone healing in a calvarial defect model\n",
      "Cosine similarity: 0.9638146758079529\n",
      "----\n",
      "Paper 594: Paroxetine versus placebo and other agents for depressive disorders: a systematic review and meta-analysis.\n",
      "Second most similar paper:\n",
      "Paper 846: The efficacy of paroxetine and placebo in treating anxiety and depression: A meta-analysis of change on the Hamilton Rating Scales\n",
      "Cosine similarity: 0.9642519354820251\n",
      "----\n",
      "Paper 747: Pharmacological treatment of depression in children and adolescents\n",
      "Second most similar paper:\n",
      "Paper 750: Pharmacotherapy for major depression in children and adolescents\n",
      "Cosine similarity: 0.9660271406173706\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def is_pair_already_printed(p1, p2, printed_pairs):\n",
    "    \"\"\"Check if the pair has already been printed.\"\"\"\n",
    "    return (p1, p2) in printed_pairs or (p2, p1) in printed_pairs\n",
    "\n",
    "\n",
    "def print_second_most_similar(df, cosine_similarity_matrix):\n",
    "    printed_pairs = set()\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        sorted_indices = np.argsort(cosine_similarity_matrix[i])\n",
    "        second_most_similar = sorted_indices[-2]  # -1 is the most similar (itself)\n",
    "\n",
    "        # Skip if the pair has already been printed\n",
    "        if is_pair_already_printed(i, second_most_similar, printed_pairs):\n",
    "            continue\n",
    "\n",
    "        # Handle the case where the most similar paper might be the paper itself\n",
    "        if second_most_similar == i:\n",
    "            second_most_similar = sorted_indices[-3]\n",
    "\n",
    "        # Check if the similarity is above the threshold\n",
    "        if cosine_similarity_matrix[i][second_most_similar] > 0.96:\n",
    "            printed_pairs.add((i, second_most_similar))\n",
    "\n",
    "            print(f\"Paper {i}: {row['title']}\")\n",
    "            print(\"Second most similar paper:\")\n",
    "            print(\n",
    "                f\"Paper {second_most_similar}: {df.iloc[second_most_similar]['title']}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Cosine similarity: {cosine_similarity_matrix[i][second_most_similar]}\"\n",
    "            )\n",
    "            print(\"----\")\n",
    "\n",
    "\n",
    "print_second_most_similar(processor.df, cosine_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
