{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Reference Data from Scopus API\n",
    "\n",
    "Resouces:\n",
    "\n",
    "- [Scopus Abstract Retrieval Views](https://dev.elsevier.com/sc_abstract_retrieval_views.html)\n",
    "- [Scopus Retrieval API](https://dev.elsevier.com/documentation/AbstractRetrievalAPI.wadl)\n",
    "- [Interactive Scopus API](https://dev.elsevier.com/scopus.html)\n",
    "- [API Settings (rate limits)](https://dev.elsevier.com/api_key_settings.html)\n",
    "- Remember Logging In to Cisco VPN!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the `ScopusReferenceFetech` (from src.data.ScopusReferenceFetcher import ScopusReferenceFetcher). The class was created to send requests to the scopus search api and retrieve the references for a given scopus id.\n",
    "\n",
    "In this class, we are applying the functionility to a list of scopus ids.\n",
    "\n",
    "1. We provide the full dataframe with the scopus ids - the code runs until the api rate limit of 10,000 requests per week is reached.\n",
    "2. It saves after every 500 requests.\n",
    "3. After the first run, and the first rate limit is reached, we use addittional api keys to continue the process.\n",
    "4. To do that, we first load all previously saved files from the data folder.\n",
    "5. We save the 'eid's of the files that were already processed in a list to filter the dataframe to those that are not yet processed.\n",
    "6. We retrieve the highest number of batches to know where to continue from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "from math import ceil\n",
    "import sys\n",
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "import sys\n",
    "\n",
    "\n",
    "from src.data.ScopusReferenceFetcher import ScopusReferenceFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScopusRefFetcherPrep:\n",
    "    \"\"\"\n",
    "    Class to prepare the data for fetching references from Scopus.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_api_keys(path=\"../notebooks/api_key_scopus.json\"):\n",
    "        api_keys = json.load(open(path))\n",
    "        return api_keys\n",
    "\n",
    "    @staticmethod\n",
    "    def load_fetched_reference_data(data_path):\n",
    "        \"\"\"\n",
    "        This is only necessary after the inital fetch of the reference data.\n",
    "        It loads the data and returns a list of EIDs and the highest batch number.\n",
    "        \"\"\"\n",
    "        files = os.listdir(data_path)\n",
    "        files = [file for file in files if file.endswith(\".json\")]\n",
    "        # Extract batch numbers and find the maximum\n",
    "        max_batch = max(int(file.split(\"_\")[3].split(\".\")[0]) for file in files)\n",
    "        print(f\"Found {len(files)} files with batch numbers up to {max_batch}.\")\n",
    "        eids = []\n",
    "        for file in files:\n",
    "            with open(data_path + file, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                eids.extend(list(data.keys()))\n",
    "        # remove data from memory\n",
    "        del data\n",
    "        return eids, max_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_filter_articles(path, eids):\n",
    "        df = pd.read_csv(path)\n",
    "        df_filtered = df[~df[\"eid\"].isin(eids)]\n",
    "        df_filtered = df_filtered.reset_index(drop=True)\n",
    "        print(f\"Number of articles to fetch: {len(df_filtered)}\")\n",
    "        return df_filtered\n",
    "\n",
    "\n",
    "class ScopusRefFetcherProcessor:\n",
    "    \"\"\"\n",
    "    Class to fetch references from Scopus.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_logging(log_directory, log_level=logging.INFO):\n",
    "        \"\"\"\n",
    "        Set up logging to write to a file with a timestamp in its name.\n",
    "\n",
    "        Args:\n",
    "        log_directory (str): Directory where the log file will be saved.\n",
    "        log_level (logging.Level): Logging level to capture. Default is logging.INFO.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a directory for logs if it doesn't exist\n",
    "        os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "        # Configure logging to write to a file with a timestamp in its name\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        log_filename = f\"{log_directory}/scopus_fetcher_{current_time}.log\"\n",
    "        logging.basicConfig(\n",
    "            filename=log_filename,\n",
    "            level=log_level,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def process_scopus_batches(\n",
    "        api_key, df_to_fetch, data_path, last_processed_batch, batch_size=500\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Process batches of data and fetch references using ScopusReferenceFetcher.\n",
    "\n",
    "        Args:\n",
    "        api_key (str): API key for ScopusReferenceFetcher.\n",
    "        df_to_fetch (DataFrame): DataFrame containing data to be processed.\n",
    "        data_path (str): Path where the processed data will be saved.\n",
    "        last_processed_batch (int): The last processed batch number for naming files.\n",
    "        batch_size (int, optional): Number of records to process in each batch. Default is 500.\n",
    "        \"\"\"\n",
    "\n",
    "        fetcher = ScopusReferenceFetcher(api_key)\n",
    "        num_batches = ceil(len(df_to_fetch) / batch_size)\n",
    "\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            data_dict = {}\n",
    "\n",
    "            for _, row in df_to_fetch.iloc[\n",
    "                batch * batch_size : (batch + 1) * batch_size\n",
    "            ].iterrows():\n",
    "                try:\n",
    "                    data_dict[row[\"eid\"]] = fetcher.request_eid(row[\"eid\"])\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing EID: {row['eid']}. Error: {e}\")\n",
    "                    if \"429\" in str(e):\n",
    "                        logging.error(\"Too many requests, breaking the loop.\")\n",
    "                        with open(\n",
    "                            f\"{data_path}scopus_references_batch_{last_processed_batch + batch + 1}.json\",\n",
    "                            \"w\",\n",
    "                        ) as fp:\n",
    "                            json.dump(data_dict, fp)\n",
    "                        logging.info(\n",
    "                            f\"Last possible batch {last_processed_batch + batch + 1} saved uncompleted.\"\n",
    "                        )\n",
    "                        sys.exit(\"Stopping script due to 429 Too Many Requests error.\")\n",
    "\n",
    "            with open(\n",
    "                f\"{data_path}scopus_references_batch_{last_processed_batch + batch + 1}.json\",\n",
    "                \"w\",\n",
    "            ) as fp:\n",
    "                json.dump(data_dict, fp)\n",
    "            logging.info(\n",
    "                f\"Data for batch {last_processed_batch + batch + 1} saved to file.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Reference Fetching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get api key and load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first api key\n",
    "api_keys = ScopusRefFetcherPrep.get_api_keys()\n",
    "api_key = api_keys[\"api_key_A\"]\n",
    "\n",
    "# load data\n",
    "df_path = \"../data/01-raw/scopus/articles/final_scopus_results.csv\"\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup logging and process the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = \"../notebooks/logs/scopus_fetcher\"\n",
    "ScopusRefFetcherProcessor.setup_logging(log_directory, log_level=logging.INFO)\n",
    "\n",
    "# max batch is 0 since no data has been fetched yet\n",
    "max_batch = 0\n",
    "# Process batches\n",
    "processor.process_scopus_batches(\n",
    "    api_key=api_key,\n",
    "    df_to_fetch=df,\n",
    "    data_path=\"../data/01-raw/scopus/references/\",\n",
    "    last_processed_batch=max_batch,\n",
    "    batch_size=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsequent Reference Fetching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78 files with batch numbers up to 77.\n",
      "Number of articles to fetch: 3744\n"
     ]
    }
   ],
   "source": [
    "prepper = ScopusRefFetcherPrep()\n",
    "api_keys = prepper.get_api_keys()\n",
    "api_key = api_keys[\"api_key_deb\"]\n",
    "\n",
    "refs_path = \"../data/01-raw/scopus/references/\"\n",
    "eids, max_batch = prepper.load_fetched_reference_data(refs_path)\n",
    "\n",
    "df_path = \"../data/01-raw/scopus/articles/final_scopus_results.csv\"\n",
    "df_filtered = prepper.load_and_filter_articles(df_path, eids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import class\n",
    "from src.data.ScopusReferenceFetcher import ScopusReferenceFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [1:41:40<00:00, 762.51s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = ScopusRefFetcherProcessor()\n",
    "\n",
    "# Set up logging\n",
    "log_directory = \"../notebooks/logs/scopus_fetcher\"\n",
    "processor.setup_logging(log_directory, log_level=logging.INFO)\n",
    "\n",
    "# Process batches\n",
    "processor.process_scopus_batches(\n",
    "    api_key=api_key,\n",
    "    df_to_fetch=df_filtered,\n",
    "    data_path=\"../data/01-raw/scopus/references/\",\n",
    "    last_processed_batch=max_batch,\n",
    "    batch_size=500,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
