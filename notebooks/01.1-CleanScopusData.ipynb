{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Scopus Publications\n",
    "\n",
    "This notebook performs data cleaning and preprocessing on the Scopus publications dataset. The main steps include:\n",
    "\n",
    "1. Loading and initial data exploration\n",
    "2. Data cleaning using ScopusDataCleaner class\n",
    "3. Data quality checks and validation\n",
    "4. Visualization of cleaned data\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/jlq293/Projects/Study-1-Bibliometrics\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from src.data_fetching.ScopusDataCleaner import ScopusDataCleaner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data\n",
    "\n",
    "First, we load the raw Scopus data from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44444, 37)\n"
     ]
    }
   ],
   "source": [
    "p = \"../data/01-raw/scopusnew/final_scopus_results_20250326_081230.csv\"\n",
    "df = pd.read_csv(p)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We use the ScopusDataCleaner class to perform the following cleaning steps:\n",
    "\n",
    "1. Drop unnecessary columns\n",
    "2. Rename columns for consistency\n",
    "3. Filter publication types\n",
    "4. Remove duplicates\n",
    "5. Format dates\n",
    "6. Create unique author-year combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.ScopusDataCleaner:Initialized with 44444 rows and 37 columns\n",
      "INFO:src.data.ScopusDataCleaner:Dropped 12 columns\n",
      "INFO:src.data.ScopusDataCleaner:Columns renamed successfully\n",
      "INFO:src.data.ScopusDataCleaner:Removing 600 Book publications\n",
      "INFO:src.data.ScopusDataCleaner:Removing 238 Book Series publications\n",
      "INFO:src.data.ScopusDataCleaner:Removing 92 Conference Proceeding publications\n",
      "INFO:src.data.ScopusDataCleaner:Removing 27 Trade Journal publications\n",
      "INFO:src.data.ScopusDataCleaner:Remaining publications: 43473\n",
      "INFO:src.data.ScopusDataCleaner:Removing 757 Conference Paper publications\n",
      "INFO:src.data.ScopusDataCleaner:Remaining publications: 42716\n",
      "INFO:src.data.ScopusDataCleaner:Removed 0 duplicates based on eid\n",
      "INFO:src.data.ScopusDataCleaner:Removed 27 duplicates based on abstract\n",
      "INFO:src.data.ScopusDataCleaner:Removed 3728 rows with missing abstracts\n",
      "INFO:src.data.ScopusDataCleaner:Date columns formatted successfully\n",
      "INFO:src.data.ScopusDataCleaner:Created unique_auth_year column\n"
     ]
    }
   ],
   "source": [
    "scopus_cleaner = ScopusDataCleaner(df)\n",
    "scopus_cleaner.drop_columns()\n",
    "scopus_cleaner.rename_columns()\n",
    "scopus_cleaner.subset_publication_type()\n",
    "scopus_cleaner.subset_publication_subtype()\n",
    "scopus_cleaner.remove_duplicates(column=\"eid\")\n",
    "scopus_cleaner.remove_duplicates(column=\"abstract\")\n",
    "scopus_cleaner.remove_missing_abstracts()\n",
    "\n",
    "scopus_cleaner.date_formater()\n",
    "scopus_cleaner.unique_auth_year_col()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cleaned Data and Removal Log\n",
    "\n",
    "We retrieve the cleaned dataset and the removal log to track changes made during cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (38961, 28)\n"
     ]
    }
   ],
   "source": [
    "df = scopus_cleaner.get_dataframe()\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "removal_log = scopus_cleaner.get_removal_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "\n",
    "Save the cleaned dataset and removal log for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as pkl\n",
    "p = \"../data/02-clean/articles/scopus_cleaned_20250326_081230.pkl\"\n",
    "df.to_pickle(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save removal log as json\n",
    "p = \"../output/descriptive-stats-logs/\"\n",
    "# Saving the dictionary to a JSON file\n",
    "# Convert numpy types to Python native types\n",
    "converted_dict = {\n",
    "    k: (v.item() if isinstance(v, np.generic) else v) for k, v in removal_log.items()\n",
    "}\n",
    "\n",
    "# Saving the dictionary to a JSON file\n",
    "with open(p + \"scopus_removal_log_20250326_081230.json\", \"w\") as f:\n",
    "    json.dump(converted_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibliometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
