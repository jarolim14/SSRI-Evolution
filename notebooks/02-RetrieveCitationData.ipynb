{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Reference Data from Scopus API\n",
    "\n",
    "Resouces:\n",
    "\n",
    "- [Scopus Abstract Retrieval Views](https://dev.elsevier.com/sc_abstract_retrieval_views.html)\n",
    "- [Scopus Retrieval API](https://dev.elsevier.com/documentation/AbstractRetrievalAPI.wadl)\n",
    "- [Interactive Scopus API](https://dev.elsevier.com/scopus.html)\n",
    "- [API Settings (rate limits)](https://dev.elsevier.com/api_key_settings.html)\n",
    "- Remember Logging In to Cisco VPN!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the `ScopusReferenceFetech` (from src.data_fetching.ScopusReferenceFetcher import ScopusReferenceFetcher). The class was created to send requests to the scopus search api and retrieve the references for a given scopus id.\n",
    "\n",
    "In this class, we are applying the functionility to a list of scopus ids.\n",
    "\n",
    "1. We provide the full dataframe with the scopus ids - the code runs until the api rate limit of 10,000 requests per week is reached.\n",
    "2. It saves after every 500 requests.\n",
    "3. After the first run, and the first rate limit is reached, we use addittional api keys to continue the process.\n",
    "4. To do that, we first load all previously saved files from the data folder.\n",
    "5. We save the 'eid's of the files that were already processed in a list to filter the dataframe to those that are not yet processed.\n",
    "6. We retrieve the highest number of batches to know where to continue from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "from math import ceil\n",
    "import sys\n",
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from src.data_fetching.ScopusReferenceFetcher import ScopusReferenceFetcher\n",
    "from src.data_fetching.ScopusReferenceFetcherUtils import (\n",
    "    ScopusRefFetcherPrep,\n",
    "    ScopusRefFetcherProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Reference Fetching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get api key and load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment:  rate limits are 40,000 per week for api_key_A and 10,000 for every other key\n"
     ]
    }
   ],
   "source": [
    "api_keys = ScopusRefFetcherPrep.get_api_keys()\n",
    "api_key = api_keys[\"api_key_A\"]\n",
    "\n",
    "# load data\n",
    "df_path = \"../data/02-clean/articles/scopus_cleaned_20250326_081230.pkl\"\n",
    "df = pd.read_pickle(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup logging and process the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = \"../data/02-clean/references/\"\n",
    "ScopusRefFetcherProcessor.setup_logging(log_directory, log_level=logging.INFO)\n",
    "\n",
    "# max batch is 0 since no data has been fetched yet\n",
    "max_batch = 0\n",
    "# Process batches\n",
    "processor.process_scopus_batches(\n",
    "    api_key=api_key,\n",
    "    df_to_fetch=df,\n",
    "    data_path=\"../data/01-raw/scopus/references/\",\n",
    "    last_processed_batch=max_batch,\n",
    "    batch_size=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsequent Reference Fetching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78 files with batch numbers up to 77.\n",
      "Number of articles to fetch: 3744\n"
     ]
    }
   ],
   "source": [
    "prepper = ScopusRefFetcherPrep()\n",
    "api_keys = prepper.get_api_keys()\n",
    "api_key = api_keys[\"api_key_deb\"]\n",
    "\n",
    "refs_path = \"../data/01-raw/scopus/references/\"\n",
    "eids, max_batch = prepper.load_fetched_reference_data(refs_path)\n",
    "\n",
    "df_path = \"../data/01-raw/scopus/articles/final_scopus_results.csv\"\n",
    "df_filtered = prepper.load_and_filter_articles(df_path, eids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [1:41:40<00:00, 762.51s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = ScopusRefFetcherProcessor()\n",
    "\n",
    "# Set up logging\n",
    "log_directory = \"../notebooks/logs/scopus_fetcher\"\n",
    "processor.setup_logging(log_directory, log_level=logging.INFO)\n",
    "\n",
    "# Process batches\n",
    "processor.process_scopus_batches(\n",
    "    api_key=api_key,\n",
    "    df_to_fetch=df_filtered,\n",
    "    data_path=\"../data/01-raw/scopus/references/\",\n",
    "    last_processed_batch=max_batch,\n",
    "    batch_size=500,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibliometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
