{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76faeab",
   "metadata": {
    "cellUniqueIdByVincent": "1f938"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad60239",
   "metadata": {
    "cellUniqueIdByVincent": "cfc28"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import igraph as ig\n",
    "from pandas import Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460eead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded with rows: 36510\n",
      "Graph loaded with 36510 vertices and 551227 edges.\n",
      "Cluster labels loaded with 99 entries.\n",
      "Legend loaded with 4 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/bibliometrics/lib/python3.11/site-packages/igraph/io/files.py:295: RuntimeWarning: Could not add vertex ids, there is already an 'id' vertex attribute. at src/io/graphml.c:488\n",
      "  return reader(f, *args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir: str, output_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset and reference files.\n",
    "\n",
    "    This method loads the main DataFrame, cluster labels, and legend files\n",
    "    required for the analysis. It ensures that all necessary data is available\n",
    "    for subsequent operations.\n",
    "\n",
    "    Returns:\n",
    "        self: The ClusterAnalyzer instance with loaded data.\n",
    "    \"\"\"\n",
    "    # Load DataFrame\n",
    "    pdf = os.path.join(data_dir, \"08-analysis-data/df_analysis.pkl\")\n",
    "    df = pd.read_pickle(pdf)\n",
    "    print(f\"DataFrame loaded with rows: {len(df)}\")\n",
    "\n",
    "    # Load graph\n",
    "    pg = os.path.join(data_dir, \"08-analysis-data/graph_analysis.graphml\")\n",
    "    g = ig.read(pg)\n",
    "    print(f\"Graph loaded with {g.vcount()} vertices and {g.ecount()} edges.\")\n",
    "    # Load cluster labels\n",
    "    labels_path = os.path.join(\n",
    "        output_dir,\n",
    "        \"cluster-qualifications_2025/cluster-label-tree/cluster_labels_filtered.json\",\n",
    "    )\n",
    "    with open(labels_path, \"r\") as f:\n",
    "        cluster_label_dict = json.load(f)\n",
    "    print(f\"Cluster labels loaded with {len(cluster_label_dict)} entries.\")\n",
    "\n",
    "    # Load legend\n",
    "    legend_path = os.path.join(\n",
    "        output_dir,\n",
    "        \"cluster-qualifications_2025/cluster-label-tree/legend_labels_2025.json\",\n",
    "    )\n",
    "    with open(legend_path, \"r\") as f:\n",
    "        legend = json.load(f)\n",
    "    print(f\"Legend loaded with {len(legend)} entries.\")\n",
    "    return df, g, cluster_label_dict, legend\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "output_dir = os.getenv(\"OUTPUT_DIR\")\n",
    "\n",
    "df, g, cluster_label_dict, legend = load_data(data_dir=data_dir, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35fb7e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "igraph.Edge(<igraph.Graph object at 0x1651d9e50>, 10, {'weight': 0.634322166442871})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.es[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d043b669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "igraph.Vertex(<igraph.Graph object at 0x1651d9e50>, 10, {'eid': '2-s2.0-0020144348', 'title': 'Serotonin and fear retention in the rat', 'year': 1982.0, 'id': '10', 'cluster': '47', 'centrality_alpha0.3_k10_res0.002': 0.0157127297097148, 'doi': '10.1037/h0077897', 'authors': 'Archer'})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.vs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee1d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "\n",
    "def calculate_modularity(snapshot_graph, cluster_attribute=\"cluster\"):\n",
    "    \"\"\"\n",
    "    Calculate modularity for a graph with cluster assignments.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    snapshot_graph : igraph.Graph\n",
    "        The graph to analyze\n",
    "    cluster_attribute : str\n",
    "        The vertex attribute containing cluster assignments\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float : The modularity value\n",
    "    \"\"\"\n",
    "    # Get all clusters\n",
    "    clusters = set(snapshot_graph.vs[cluster_attribute])\n",
    "\n",
    "    # Build cluster membership mapping\n",
    "    cluster_nodes = defaultdict(list)\n",
    "    for v in snapshot_graph.vs:\n",
    "        cluster_nodes[v[cluster_attribute]].append(v.index)\n",
    "\n",
    "    # Build edge matrix between clusters\n",
    "    cluster_edge_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Count internal edges for each cluster\n",
    "    internal_edges = defaultdict(int)\n",
    "\n",
    "    for edge in snapshot_graph.es:\n",
    "        source_cluster = snapshot_graph.vs[edge.source][cluster_attribute]\n",
    "        target_cluster = snapshot_graph.vs[edge.target][cluster_attribute]\n",
    "\n",
    "        if source_cluster == target_cluster:\n",
    "            internal_edges[source_cluster] += 1\n",
    "        else:\n",
    "            cluster_edge_matrix[source_cluster][target_cluster] += 1\n",
    "            cluster_edge_matrix[target_cluster][source_cluster] += 1\n",
    "\n",
    "    # Calculate modularity\n",
    "    total_edges = snapshot_graph.ecount()\n",
    "\n",
    "    if total_edges == 0:\n",
    "        return 0\n",
    "\n",
    "    modularity = 0\n",
    "\n",
    "    # Calculate degree for each cluster (sum of all edges touching nodes in cluster)\n",
    "    cluster_degrees = defaultdict(int)\n",
    "    for cluster in clusters:\n",
    "        # Internal edges count twice for degree\n",
    "        cluster_degrees[cluster] = internal_edges[cluster] * 2\n",
    "        # Add external edges\n",
    "        cluster_degrees[cluster] += sum(cluster_edge_matrix[cluster].values())\n",
    "\n",
    "    # Calculate modularity using the standard formula\n",
    "    for cluster in clusters:\n",
    "        # Actual internal edges\n",
    "        actual_internal = internal_edges[cluster]\n",
    "\n",
    "        # Expected edges under null model\n",
    "        expected_internal = (cluster_degrees[cluster] ** 2) / (4 * total_edges)\n",
    "\n",
    "        modularity += actual_internal - expected_internal\n",
    "\n",
    "    modularity = modularity / total_edges\n",
    "\n",
    "    return modularity\n",
    "\n",
    "\n",
    "def create_temporal_snapshots(graph, years, time_window_size=3):\n",
    "    \"\"\"\n",
    "    Create temporal snapshots of the graph based on time windows.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    graph : igraph.Graph\n",
    "        The full graph with 'year' vertex attribute\n",
    "    years : list\n",
    "        List of years to consider\n",
    "    time_window_size : int\n",
    "        Size of the time window in years\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with (start_year, end_year) tuples as keys\n",
    "    \"\"\"\n",
    "    time_periods = range(min(years), max(years) + 1, time_window_size)\n",
    "    temporal_data = {}\n",
    "\n",
    "    for year_start in time_periods:\n",
    "        year_end = year_start + time_window_size - 1\n",
    "\n",
    "        # Identify nodes published within this time window\n",
    "        nodes_in_window = [n for n in graph.vs if year_start <= n[\"year\"] <= year_end]\n",
    "\n",
    "        # Create snapshot graph\n",
    "        snapshot_graph = graph.subgraph(nodes_in_window).copy()\n",
    "\n",
    "        # Calculate modularity\n",
    "        modularity = calculate_modularity(snapshot_graph)\n",
    "\n",
    "        # Store results\n",
    "        temporal_data[(year_start, year_end)] = {\n",
    "            \"graph\": snapshot_graph,\n",
    "            \"n_nodes\": snapshot_graph.vcount(),\n",
    "            \"n_edges\": snapshot_graph.ecount(),\n",
    "            \"n_clusters\": len(set(snapshot_graph.vs[\"cluster\"])),\n",
    "            \"modularity\": modularity,\n",
    "        }\n",
    "\n",
    "        print(f\"\\nPeriod {year_start}-{year_end}:\")\n",
    "        print(f\"  Nodes: {snapshot_graph.vcount()}, Edges: {snapshot_graph.ecount()}\")\n",
    "        print(f\"  Clusters: {len(set(snapshot_graph.vs['cluster']))}\")\n",
    "        print(f\"  Modularity: {modularity:.4f}\")\n",
    "\n",
    "    return temporal_data\n",
    "\n",
    "\n",
    "def extract_time_series(temporal_data):\n",
    "    \"\"\"\n",
    "    Extract time series data for plotting.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    temporal_data : dict\n",
    "        Dictionary returned by create_temporal_snapshots\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with metric names as keys and lists of values\n",
    "    \"\"\"\n",
    "    time_series = {\n",
    "        \"periods\": [],\n",
    "        \"modularity\": [],\n",
    "        \"n_nodes\": [],\n",
    "        \"n_edges\": [],\n",
    "        \"n_clusters\": [],\n",
    "    }\n",
    "\n",
    "    for (year_start, year_end), data in sorted(temporal_data.items()):\n",
    "        time_series[\"periods\"].append(f\"{year_start}-{year_end}\")\n",
    "        time_series[\"modularity\"].append(data[\"modularity\"])\n",
    "        time_series[\"n_nodes\"].append(data[\"n_nodes\"])\n",
    "        time_series[\"n_edges\"].append(data[\"n_edges\"])\n",
    "        time_series[\"n_clusters\"].append(data[\"n_clusters\"])\n",
    "\n",
    "    return time_series\n",
    "\n",
    "\n",
    "# Main analysis function\n",
    "def analyze_temporal_modularity(\n",
    "    graph, years=None, time_window_size=3, plot=True, save_plot=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform complete temporal modularity analysis.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    graph : igraph.Graph\n",
    "        The graph to analyze (must have 'year' and 'cluster' vertex attributes)\n",
    "    years : list, optional\n",
    "        List of years to consider (if None, extracts from graph)\n",
    "    time_window_size : int\n",
    "        Size of time windows in years\n",
    "    plot : bool\n",
    "        Whether to create plots\n",
    "    save_plot : str, optional\n",
    "        Path to save the plot\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (temporal_data, time_series)\n",
    "    \"\"\"\n",
    "    # Extract years if not provided\n",
    "    if years is None:\n",
    "        years = list(range(min(graph.vs[\"year\"]), max(graph.vs[\"year\"]) + 1))\n",
    "\n",
    "    print(f\"Analyzing years: {min(years)} to {max(years)}\")\n",
    "    print(f\"Time window size: {time_window_size} years\")\n",
    "\n",
    "    # Create temporal snapshots\n",
    "    temporal_data = create_temporal_snapshots(graph, years, time_window_size)\n",
    "\n",
    "    # Extract time series\n",
    "    time_series = extract_time_series(temporal_data)\n",
    "\n",
    "    return temporal_data, time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7fd9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing years: 1982 to 2025\n",
      "Time window size: 3 years\n",
      "\n",
      "Period 1982-1984:\n",
      "  Nodes: 235, Edges: 274\n",
      "  Clusters: 44\n",
      "  Modularity: 0.5711\n",
      "\n",
      "Period 1985-1987:\n",
      "  Nodes: 331, Edges: 448\n",
      "  Clusters: 51\n",
      "  Modularity: 0.5578\n",
      "\n",
      "Period 1988-1990:\n",
      "  Nodes: 514, Edges: 633\n",
      "  Clusters: 62\n",
      "  Modularity: 0.6769\n",
      "\n",
      "Period 1991-1993:\n",
      "  Nodes: 1045, Edges: 1865\n",
      "  Clusters: 85\n",
      "  Modularity: 0.6021\n",
      "\n",
      "Period 1994-1996:\n",
      "  Nodes: 1606, Edges: 3237\n",
      "  Clusters: 94\n",
      "  Modularity: 0.5975\n",
      "\n",
      "Period 1997-1999:\n",
      "  Nodes: 2217, Edges: 4705\n",
      "  Clusters: 96\n",
      "  Modularity: 0.6138\n",
      "\n",
      "Period 2000-2002:\n",
      "  Nodes: 2547, Edges: 4965\n",
      "  Clusters: 98\n",
      "  Modularity: 0.6317\n",
      "\n",
      "Period 2003-2005:\n",
      "  Nodes: 2914, Edges: 5410\n",
      "  Clusters: 98\n",
      "  Modularity: 0.6631\n",
      "\n",
      "Period 2006-2008:\n",
      "  Nodes: 3449, Edges: 7008\n",
      "  Clusters: 99\n",
      "  Modularity: 0.6539\n",
      "\n",
      "Period 2009-2011:\n",
      "  Nodes: 3594, Edges: 6418\n",
      "  Clusters: 99\n",
      "  Modularity: 0.6794\n",
      "\n",
      "Period 2012-2014:\n",
      "  Nodes: 3841, Edges: 6375\n",
      "  Clusters: 99\n",
      "  Modularity: 0.6851\n",
      "\n",
      "Period 2015-2017:\n",
      "  Nodes: 4024, Edges: 7207\n",
      "  Clusters: 99\n",
      "  Modularity: 0.6868\n",
      "\n",
      "Period 2018-2020:\n",
      "  Nodes: 3969, Edges: 7171\n",
      "  Clusters: 98\n",
      "  Modularity: 0.7222\n",
      "\n",
      "Period 2021-2023:\n",
      "  Nodes: 4374, Edges: 11135\n",
      "  Clusters: 98\n",
      "  Modularity: 0.7026\n",
      "\n",
      "Period 2024-2026:\n",
      "  Nodes: 1850, Edges: 2277\n",
      "  Clusters: 98\n",
      "  Modularity: 0.6943\n"
     ]
    }
   ],
   "source": [
    "years = list(range(1982, 2026))\n",
    "temporal_data, time_series = analyze_temporal_modularity(g, years, time_window_size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibliometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vincent": {
   "sessionId": "15039f33fa58b8e5a1d6b1d2_2025-06-12T07-40-25-725Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
