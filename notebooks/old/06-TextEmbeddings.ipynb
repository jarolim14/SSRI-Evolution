{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings\n",
    "\n",
    "Model: pritamdeka/S-PubMedBert-MS-MARCO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.nlp.EmbeddingCreator import EmbeddingCreator\n",
    "from src.nlp.TextProcessor import TextProcessor\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_pickle(\"../data/processed/merged_data_refsfetched.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text\n",
    "\n",
    "remove NoTitle NoAbstract strings, extra spaces, extra punctuation, and ending statements (copytright...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function\n",
    "text_preprocessor = TextProcessor()\n",
    "df[\"title_abstract\"] = df[\"title_abstract\"].apply(\n",
    "    text_preprocessor.clean_text_and_remove_ending_statements\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using autotokenizer and automodel\n",
      "Model: pritamdeka/S-PubMedBert-MS-MARCO\n",
      "Df Shape: (40481, 20)\n",
      "Dataframe sorted by year and split into 21 chunks of size 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 137.16it/s]\n",
      "Processing batches: 100%|██████████| 63/63 [14:17<00:00, 13.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 0 to pickle: ../data/processed/embeddings/0_1983-1993.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [12:33<00:00, 11.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 to pickle: ../data/processed/embeddings/1_1993-1996.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:28<00:00, 10.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 2 to pickle: ../data/processed/embeddings/2_1996-1999.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [12:40<00:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 3 to pickle: ../data/processed/embeddings/3_1999-2001.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:45<00:00, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 4 to pickle: ../data/processed/embeddings/4_2001-2002.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [13:39<00:00, 13.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 5 to pickle: ../data/processed/embeddings/5_2002-2004.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [13:21<00:00, 12.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 6 to pickle: ../data/processed/embeddings/6_2004-2006.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [12:12<00:00, 11.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 7 to pickle: ../data/processed/embeddings/7_2006-2007.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:09<00:00, 10.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 8 to pickle: ../data/processed/embeddings/8_2007-2008.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:10<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 9 to pickle: ../data/processed/embeddings/9_2008-2010.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:10<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 10 to pickle: ../data/processed/embeddings/10_2010-2011.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:12<00:00, 10.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 11 to pickle: ../data/processed/embeddings/11_2011-2012.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:12<00:00, 10.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 12 to pickle: ../data/processed/embeddings/12_2012-2014.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:12<00:00, 10.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 13 to pickle: ../data/processed/embeddings/13_2014-2015.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:10<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 14 to pickle: ../data/processed/embeddings/14_2015-2016.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:14<00:00, 10.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 15 to pickle: ../data/processed/embeddings/15_2016-2017.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:18<00:00, 10.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 16 to pickle: ../data/processed/embeddings/16_2017-2019.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:15<00:00, 10.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 17 to pickle: ../data/processed/embeddings/17_2019-2020.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:14<00:00, 10.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 18 to pickle: ../data/processed/embeddings/18_2020-2021.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 63/63 [11:32<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 19 to pickle: ../data/processed/embeddings/19_2021-2022.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 16/16 [02:49<00:00, 10.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 20 to pickle: ../data/processed/embeddings/20_2022-9999.pkl\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "model_path = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "\n",
    "# Create an instance of TextProcessor\n",
    "embedding_creator = EmbeddingCreator(df, modelpath=model_path, batch_size=32)\n",
    "\n",
    "# Process and save the chunks\n",
    "df_embeddings = embedding_creator.create_embeddings(\n",
    "    text_column_name=\"title_abstract\",\n",
    "    embeddings_column_name=\"embeddings_biomedbert_sentence_transformer\",\n",
    "    save_directory=\"../data/processed/embeddings\",\n",
    "    return_df=True,\n",
    "    start_chunk=0,\n",
    "    chunk_size=2000,\n",
    "    max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "p = \"/Users/jlq293/Projects/Study-1-Bibliometrics/data/processed/embeddings/\"\n",
    "\n",
    "# read and merge all pkl files\n",
    "df_embeddings = pd.concat(\n",
    "    [pd.read_pickle(p + f) for f in os.listdir(p) if f.endswith(\".pkl\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# cluster\n",
    "from src.models.KMeansClustering import KMeansClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.02\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 14\n",
    "embeddings_column = \"embeddings_biomedbert_sentence_transformer\"\n",
    "clusterer = KMeansClustering(\n",
    "    df=df_embeddings,\n",
    "    num_clusters=num_clusters,\n",
    "    embeddings_column=embeddings_column,\n",
    ")\n",
    "\n",
    "clusterer.kmeans_clustering(\n",
    "    # save_path=\"../data/processed/embeddings/kmeans_clustered.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = clusterer.df.groupby(\"cluster\").apply(\n",
    "    lambda x: x.nsmallest(15, \"distance_to_centroid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(num_clusters):\n",
    "    with open(f\"../data/processed/embeddings/cluster_{cluster}.txt\", \"w\") as f:\n",
    "        f.write(\"Cluster \" + str(cluster) + \"\\n\")\n",
    "        f.write(\" --------------------------------------------- \\n\")\n",
    "\n",
    "        # Filter the DataFrame for the current cluster\n",
    "        cluster_rows = top_n[top_n[\"cluster\"] == cluster]\n",
    "\n",
    "        for i, row in cluster_rows.iterrows():\n",
    "            f.write(str(row[\"title\"]) + \"\\n\")\n",
    "            f.write(str(row[\"abstract\"]) + \"\\n\")\n",
    "            f.write(str(row[\"paper_id\"]) + \"\\n\")  # Convert to string if necessary\n",
    "            f.write(str(row[\"distance_to_centroid\"]) + \"\\n\")  # Convert to string\n",
    "            f.write(\"\\n --------------------------------------------- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
